{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DT logs prediction model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Problem description](#Problem-description)\n",
    "* [Dataset](#Dataset)\n",
    "* [Model architecture](#Model-architecture)\n",
    "* [Model training](#Model-training)\n",
    "* [Inference](#Inference)\n",
    "* [Metrics evaluation](#Metrics-evaluation)\n",
    "* [Conclusion](#Conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict DT logs values based on other logs values and depth info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=1\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=1\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import pickle\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.insert(0, \"/notebooks/goryachev/petroflow\")\n",
    "\n",
    "from petroflow import WellDataset, WS\n",
    "from petroflow.batchflow import Pipeline, B, V, C\n",
    "from petroflow.batchflow.research import Research, PrintLogger\n",
    "from petroflow.batchflow.models.torch import UNet\n",
    "from petroflow.models.logs_prediction.utils import build_dataset, calc_metrics, batch_mse, moving_average_1d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A datased of 608 wells with filtered data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILTERED_DATASET_PATH = \"../data/filtered/*\"\n",
    "filtered_dataset = WellDataset(path=FILTERED_DATASET_PATH, dirs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DT values are predicted by GK, NKTD and GZ1 logs and depth info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUTS_COL = ['GK', 'NKTD', 'GZ1', \"DEPTH KM\"]\n",
    "TARGET_COL = ['DT']\n",
    "PROPER_COL = INPUTS_COL + TARGET_COL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 crops of length 6.4m will be sampled from each well in a batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CROPS = 8\n",
    "CROP_SIZE = 64\n",
    "REINDEXATION_STEP = 0.1\n",
    "CROP_LENGTH = CROP_SIZE * REINDEXATION_STEP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split logs by non nan segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/pandas/core/frame.py:3940: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "split_pipeline = filtered_dataset >> Pipeline().drop_nans()\n",
    "batch = split_pipeline.next_batch(filtered_dataset.size)\n",
    "dataset = build_dataset(batch)\n",
    "dataset.split(shuffle=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UNet model is used for logs prediction (https://analysiscenter.github.io/batchflow/api/batchflow.models.torch.unet.html).\n",
    "\n",
    "Model configuration:\n",
    "* input shape - [4, 64] (3 types of logs and depth info)\n",
    "* output shape - [1, 64] - (DT logs)\n",
    "* the number of filters in encoder and corresponding decoder blocks - [64, 128, 256, 512, 1024]\n",
    "* each encoder and decoder block has \"cna cna\" layout with a kernel size of 3 and a ReLU activation\n",
    "* downsampling in the encoder is performed by a max pooling operation with a kernel size and a stride of 2\n",
    "* upsampling in the decoder is performed by a transposed convolution with a kernel size of 4 and a stride of 2, followed by batch normalization and a ReLU activation.\n",
    "\n",
    "Adam optimizer with default parameters is used for model training. Mean-squared error is used as a loss function.\n",
    "\n",
    "Model configuration specification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUTS_SIZE = len(INPUTS_COL)\n",
    "TARGET_SIZE = len(TARGET_COL)\n",
    "\n",
    "model_config = {\n",
    "    'initial_block/inputs': 'inputs',\n",
    "    'inputs/inputs': {'shape': [INPUTS_SIZE, CROP_SIZE]},\n",
    "    'inputs/target': {'shape': [TARGET_SIZE, CROP_SIZE]},\n",
    "    'head/num_classes': 1,\n",
    "    'loss': 'mse',\n",
    "    'device' : C('device'),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is trained for 200 epochs with a batch size of 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_name = 'research_stop'\n",
    "def clear_previous_results(res_name):\n",
    "    if os.path.exists(res_name):\n",
    "        shutil.rmtree(res_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_template = (Pipeline()\n",
    "    .add_namespace(np)\n",
    "    .random_crop(CROP_LENGTH, N_CROPS)\n",
    "    .update(B('inputs'), WS('logs')[INPUTS_COL].values.ravel())\n",
    "    .stack(B(\"inputs\"), save_to=B(\"inputs\"))\n",
    "    .swapaxes(B(\"inputs\"), 1, 2, save_to=B(\"inputs\"))\n",
    "    .array(B(\"inputs\"), dtype=np.float32, save_to=B(\"inputs\"))\n",
    "    .update(B('target'), WS('logs')[TARGET_COL].values.ravel())\n",
    "    .stack(B(\"target\"), save_to=B(\"target\"))\n",
    "    .swapaxes(B(\"target\"), 1, 2, save_to=B(\"target\"))\n",
    "    .array(B(\"target\"), dtype=np.float32, save_to=B(\"target\"))\n",
    "    .init_variable('loss_history')\n",
    "    .init_model('dynamic', UNet, 'unet', model_config)\n",
    "    .train_model('unet', B('inputs'), B('target'), fetches='loss', save_to=V('loss_history', mode='w'))\n",
    "    .run_later(batch_size=BATCH_SIZE, shuffle=True, drop_last=True, n_epochs=None)\n",
    ")\n",
    "\n",
    "train_pipeline = dataset.train >> train_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_template = (Pipeline()\n",
    "    .add_namespace(np)\n",
    "    .crop(CROP_LENGTH, CROP_LENGTH)\n",
    "    .update(B('inputs'), WS('logs')[INPUTS_COL].values.ravel())\n",
    "    .stack(B(\"inputs\"), save_to=B(\"inputs\"))\n",
    "    .swapaxes(B(\"inputs\"), 1, 2, save_to=B(\"inputs\"))\n",
    "    .array(B(\"inputs\"), dtype=np.float32, save_to=B(\"inputs\"))\n",
    "    .update(B('target'), WS('logs')[TARGET_COL].values.ravel())\n",
    "    .stack(B(\"target\"), save_to=B(\"target\"))\n",
    "    .swapaxes(B(\"target\"), 1, 2, save_to=B(\"target\"))\n",
    "    .array(B(\"target\"), dtype=np.float32, save_to=B(\"target\"))\n",
    "    .init_variable('prediction')\n",
    "    .import_model('unet', train_pipeline)\n",
    "    .predict_model('unet', B('inputs'), fetches='predictions', save_to=B('prediction'))\n",
    "    .init_variable('loss_history')\n",
    "    .batch_mse(B(\"target\"), B(\"prediction\"), save_to=V('loss_history', mode='w'))\n",
    "    .run_later(batch_size=BATCH_SIZE, shuffle=False, drop_last=False, n_epochs=1)\n",
    ")\n",
    "\n",
    "test_pipeline = dataset.test >> test_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Research research_stop is starting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Domain updated: 0:   4%|‚ñç         | 2471/60000.0 [43:11<16:45:24,  1.05s/it]"
     ]
    }
   ],
   "source": [
    "TEST_EXECUTE_FREQ = 5000\n",
    "\n",
    "ITERATIONS = 60000\n",
    "\n",
    "clear_previous_results(res_name)\n",
    "\n",
    "research = (Research()\n",
    "            .add_pipeline(train_pipeline, variables='loss_history', name='train_pipeline')\n",
    "            .add_pipeline(test_pipeline, variables='loss_history', name='test_pipeline',\n",
    "                          import_from='train_pipeline', run=True, execute=TEST_EXECUTE_FREQ))\n",
    "\n",
    "research.run(n_iters=ITERATIONS, name=res_name, devices=[0], bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = research.load_results()\n",
    "train_loss = df[df['name'] == 'train_pipeline'][['iteration', 'loss_history']].values.T\n",
    "test_loss = df[df['name'] == 'test_pipeline'][['iteration', 'loss_history']].values.T\n",
    "\n",
    "fig = plt.figure(figsize=(20, 5))\n",
    "plt.plot(*train_loss)\n",
    "plt.plot(moving_average_1d(train_loss[1], 50), 'r')\n",
    "plt.plot(*test_loss)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss value\")\n",
    "plt.legend([\"Train loss value\", \"Its moving average\", \"Test loss value\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train loss moving average value reaches the plateau almost instantly.\n",
    "\n",
    "Test loss value doesn't change during learning process and that's really bad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference pipeline is similar to the training one, except for one major difference:\n",
    "* `random_crop` method is changed to `crop`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_template = (Pipeline()\n",
    "    .add_namespace(np)\n",
    "    .crop(CROP_LENGTH, CROP_LENGTH)\n",
    "    .update(B('inputs'), WS('logs')[INPUTS_COL].values.ravel())\n",
    "    .stack(B(\"inputs\"), save_to=B(\"inputs\"))\n",
    "    .swapaxes(B(\"inputs\"), 1, 2, save_to=B(\"inputs\"))\n",
    "    .array(B(\"inputs\"), dtype=np.float32, save_to=B(\"inputs\"))\n",
    "    .update(B('target'), WS('logs')[TARGET_COL].values.ravel())\n",
    "    .stack(B(\"target\"), save_to=B(\"target\"))\n",
    "    .swapaxes(B(\"target\"), 1, 2, save_to=B(\"target\"))\n",
    "    .array(B(\"target\"), dtype=np.float32, save_to=B(\"target\"))\n",
    "    .init_variable('targets', default=[])\n",
    "    .update(V('targets', mode='a'), B('target'))\n",
    "    .init_variable('predictions', default=[])\n",
    "    .import_model('unet', train_pipeline)\n",
    "    .update_config({'device': torch.device('cuda: 0')})\n",
    "    .predict_model('unet', B('inputs'), fetches='predictions', save_to=V('predictions', mode='a'))\n",
    "    .run(batch_size=1, shuffle=False, drop_last=False, lazy=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two metrics used for model evaluation:\n",
    "* Mean squared error (MSE)\n",
    "* Proportion of variance explained by model to data variance (R^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pipeline = dataset.test >> test_template\n",
    "test_pipeline.run(n_iters=1)\n",
    "\n",
    "true = np.concatenate([target.flatten() for target in test_pipeline.v('targets')])\n",
    "pred = np.concatenate([prediction.flatten() for prediction in test_pipeline.v('predictions')])\n",
    "\n",
    "metrics = calc_metrics(true, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot several randomly chosen predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_NUM = np.random.randint(len(test_pipeline.v('predictions')))\n",
    "GRAPH_NUM = test_pipeline.v('predictions')[BATCH_NUM].shape[0]\n",
    "PRINT_NUM = 3\n",
    "for crop_num in np.random.choice(GRAPH_NUM, PRINT_NUM, replace=False):\n",
    "    true = test_pipeline.v('targets')[BATCH_NUM][crop_num, 0, :]\n",
    "    pred = test_pipeline.v('predictions')[BATCH_NUM][crop_num, 0,:]\n",
    "\n",
    "    fig = plt.figure(figsize=(15, 4))\n",
    "    plt.title(\"Crop num {}\".format(crop_num))\n",
    "    plt.plot(true, 'g')\n",
    "    plt.plot(pred, 'r')\n",
    "    plt.legend(['true', 'pred'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
