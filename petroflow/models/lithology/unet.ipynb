{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lithology prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firtsly, nesessary imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "import dill\n",
    "import datetime\n",
    "\n",
    "sys.path.insert(0, os.path.join(\"..\", \"..\", \"..\"))\n",
    "\n",
    "from petroflow import WellDataset, WS\n",
    "from petroflow.batchflow.models.torch import UNet\n",
    "from petroflow.batchflow import DatasetIndex, FilesIndex, ImagesBatch, Pipeline, V, B, W, L, R, P\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define constants that will be used at train and inference stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "N_CROPS = 4\n",
    "\n",
    "CROPS_BATCH = BATCH_SIZE * N_CROPS\n",
    "\n",
    "N_EPOCH = 500\n",
    "LENGTH = 0.1\n",
    "SHAPE = (3, int(2500 * LENGTH), 250)\n",
    "\n",
    "FILTERS = ((2 ** np.arange(4)) * 4).tolist()\n",
    "\n",
    "PATH = '/notebooks/data/september_dataset/core_photo/*/*'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove wells without `core_lithology`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = WellDataset(index=FilesIndex(path=PATH, dirs=True))\n",
    "ds = filter_dataset(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get list of classes and construct mapping from classes to labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = get_classes(ds)\n",
    "reverse_mapping = dict(enumerate(classes))\n",
    "mapping = {value: key for key, value in reverse_mapping.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preload all the data to make training process faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "load_ppl = (ds.p\n",
    "    .create_segments(src='samples', connected=True)\n",
    "    .create_segments(src='core_lithology', connected=True)\n",
    "    .drop_short_segments(LENGTH)\n",
    "    .load_core(pixels_per_cm=25)\n",
    "    .next_batch(len(ds), n_epochs=1)\n",
    ")\n",
    "\n",
    "ds = build_dataset(load_ppl)\n",
    "ds.split(shuffle=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_template = (Pipeline()\n",
    "       .add_namespace(np)\n",
    "       .copy()\n",
    "       .random_crop(length=LENGTH, n_crops=N_CROPS)\n",
    "       .update(WS('core_lithology')['CLASS'],\n",
    "               WS('core_lithology')[['FORMATION', 'GRAIN']].apply(concat, axis=1).ravel())\n",
    "       .create_mask(src='core_lithology', column='CLASS', mapping=mapping, mode='core')\n",
    "       .update(B('core'), WS('core_dl').ravel())\n",
    "       .update(B('masks'), WS('mask').ravel())\n",
    "       .array(B('core'), save_to=B('core'))\n",
    "       .array(B('masks'), save_to=B('masks'))\n",
    "       .reshape(B('masks'), (-1, 1, 250), save_to=B('masks'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation_template = (\n",
    "    Pipeline()\n",
    "    .update(B().index, L(DatasetIndex)(L(len)(B('core'))))\n",
    "    .rebatch(CROPS_BATCH, batch_class=ImagesBatch, components=('core', 'masks'))\n",
    "    .add_namespace(np)\n",
    "    .to_pil(src='core', dst='core')\n",
    "    .scale(src='core', dst='core', preserve_shape=True, factor=P(R('uniform', low=1, high=1.5)))\n",
    "    .cutout(shape=P(R('randint', low=[200, 0], high=[250, 40])),\n",
    "            origin=P(R('uniform', size=2)), color=0,\n",
    "            src='core', dst='core', p=0.5)\n",
    "    .multiply(src='core', dst='core', multiplier=P(R('uniform', low=0.7, high=1.2)))\n",
    "    .to_array(src='core', dst='core', dtype='float32')\n",
    "    .transpose(B('core'), axes=(0, 3, 1, 2), save_to=B('core'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    \"body/encoder/num_stages\": len(FILTERS[:-1]),\n",
    "    'body/encoder/blocks/filters': FILTERS[:-1],\n",
    "    \"body/decoder/blocks/filters\": FILTERS[-2::-1],\n",
    "    \"initial_block/inputs\": \"inputs\",\n",
    "    \"inputs/inputs/shape\": SHAPE,\n",
    "    'inputs/masks/shape': (len(mapping), 1, SHAPE[1]),\n",
    "    \"head\": dict(layout=\"c\",\n",
    "                 kernel_size=(SHAPE[2], 1), padding='valid', conv=dict(bias=True)),\n",
    "    \"loss\": \"ce\",\n",
    "    \"optimizer\": {\"name\": \"Adam\", \"lr\": 0.01},\n",
    "    \"output\": 'proba',\n",
    "    'device': 'gpu:1',\n",
    "}\n",
    "        \n",
    "train_template = (Pipeline()\n",
    "    .init_variable('loss_history', default=[])\n",
    "    .init_model('dynamic', UNet, 'model', model_config)\n",
    "    .train_model('model', B('core').astype('float32'), B('masks'),\n",
    "                 fetches='loss', save_to=V('loss_history', mode='a'))\n",
    ")\n",
    "\n",
    "train_ppl = (crop_template + augmentation_template + train_template) << ds.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ppl.run(BATCH_SIZE, n_epochs=N_EPOCH, bar=True, bar_desc=W(V('loss_history')[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dump results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_TO = './models/unet_' + str(datetime.datetime.now()).replace(' ', '_')\n",
    "dump_results(train_ppl, SAVE_TO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = get_last_model_path('./models/unet_*')\n",
    "print(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(model_path, 'loss.pkl'), 'rb') as f:\n",
    "    loss = dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "plt.plot(loss)\n",
    "plt.plot(pd.Series(loss).rolling(window=100).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_template(length, random_crop=False, step=None, n_crops=None):\n",
    "    step = step or length\n",
    "    n_crops = n_crops or 4\n",
    "\n",
    "    if random_crop:\n",
    "        ppl = Pipeline().random_crop(length=length, n_crops=n_crops)\n",
    "    else:\n",
    "        ppl = Pipeline().crop(length=length, step=step)\n",
    "\n",
    "    ppl = ppl + (Pipeline()\n",
    "        .add_namespace(np)\n",
    "        .copy()\n",
    "        .add_components(('core', 'masks'))\n",
    "        .update(WS('core_lithology')['CLASS'], WS('core_lithology')[['FORMATION', 'GRAIN']].apply(concat, axis=1).ravel())\n",
    "        .create_mask(src='core_lithology', column='CLASS', mapping=mapping, mode='core').update(B('core'), WS('core_dl').ravel())\n",
    "        .update(B('masks'), WS('mask').ravel())\n",
    "        .array(B('core'), save_to=B('core'))\n",
    "        .array(B('masks'), save_to=B('masks'))\n",
    "        .transpose(B('core'), axes=(0, 3, 1, 2), save_to=B('core'))\n",
    "        .reshape(B('masks'), (-1, 1, 250), save_to=B('masks'))\n",
    "        .update(B().index, L(DatasetIndex)(B('core').shape[0]))\n",
    "        .rebatch(32, components=('core', 'masks'), batch_class=ImagesBatch)\n",
    "        .init_variable('metrics')\n",
    "        .add_namespace(np)\n",
    "        .init_model('dynamic', UNet, 'model', config={\n",
    "                        'device': 'gpu:1', 'load/path': 'unet.torch'\n",
    "                    })\n",
    "        .predict_model('model', B('core').astype('float32'), fetches='proba', save_to=B('proba'))\n",
    "        .gather_metrics('classification', targets=B('masks').reshape(-1),\n",
    "                        predictions=B('proba').argmax(1).reshape(-1),\n",
    "                        fmt='labels', num_classes=len(mapping), save_to=V('metrics', mode='u'))\n",
    "    )\n",
    "    return ppl\n",
    "\n",
    "test_ppl = test_template(LENGTH, random_crop=False) << ds.test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ppl.run(10, bar=True)\n",
    "\n",
    "dump_metrics(test_ppl, os.path.join(SAVE_TO, 'metrics.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(model_path, 'metrics.pkl'), 'rb') as f:\n",
    "    metrics = dill.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1-scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, item in enumerate(metrics.evaluate('f1_score', agg='mean', multiclass=None)):\n",
    "    print(reverse_mapping[i], item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = (test_template(LENGTH, random_crop=True) << ds.test).next_batch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_examples(batch, reverse_mapping)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
