{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import time\n",
    "import collections\n",
    "from functools import reduce\n",
    "\n",
    "sys.path.insert(0, os.path.join(\"..\", \"..\", \"..\"))\n",
    "\n",
    "from petroflow import CoreBatch, CoreIndex\n",
    "from petroflow.batchflow.models.torch import ResNet18\n",
    "from petroflow.batchflow import Dataset, Pipeline, V, B, W, F, C\n",
    "from petroflow.batchflow.research import Research, Option, KV\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '/notebooks/data/august_dataset/cropped_wells'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "N_EPOCH = 50\n",
    "SHAPE = (3, 500, 250)\n",
    "\n",
    "model_config = {'initial_block/inputs': 'images',\n",
    "                'inputs/images/shape': SHAPE,\n",
    "                'inputs/labels/classes': F(lambda x: len(x.pipeline.config['labels_mapping'])),\n",
    "                'initial_block/inputs': 'images',\n",
    "                'optimizer': 'Adam',\n",
    "                'output': 'proba',\n",
    "                'device': C('device'),\n",
    "                'loss': 'ce'#, dict(weight=V('weight')))}\n",
    "               }\n",
    "\n",
    "train_ppl = (Pipeline()\n",
    "    .add_namespace(utils)\n",
    "    .set_dataset(C('dataset'))\n",
    "    .load(uv=False, dst=['dl'])\n",
    "    .normalize(src='dl', dst='dl')\n",
    "    .add_namespace(np)\n",
    "    .create_labels(C('annotation').LITHOLOGY.loc)\n",
    "    .update(B('labels'), B('labels').tolist())\n",
    "    .to_array(src='dl', dst='images', dtype='float32')\n",
    "    .make_random_crops(src=['images'], dst=['images'], channels='last', shape=SHAPE[1:], n_crops=1)\n",
    "    .concatenate(B('images'), axis=0, save_to=B('images'))\n",
    "    .init_variable('loss', default=None)\n",
    "    .transpose(B('images'), axes=(0, 3, 1, 2), save_to=B('images'))\n",
    "    .encode(B('labels'), C('labels_mapping'), save_to=B('labels'))\n",
    "    .init_model('dynamic', ResNet18, 'model', model_config)\n",
    "    .train_model('model', B('images'), B('labels'), use_lock=True, fetches='loss',\n",
    "             save_to=V('loss', mode='w'))\n",
    "    .run_later(64, n_epochs=100, shuffle=42)\n",
    ")\n",
    "\n",
    "def init_research(iteration, experiment, ppl_train, ppl_test):\n",
    "    ppl = experiment[ppl_train].pipeline\n",
    "    cfg = experiment[ppl_train].config.config()\n",
    "    ann, ds_train, ds_test, _ = utils.get_input_data(PATH, cfg['lithology'])\n",
    "    labels_mapping, reverse_mapping, counter, weights = utils.get_statistics(ds_train, ann)\n",
    "    #labels_mapping = {v: k for k,v in enumerate(cfg['lithology'])}\n",
    "\n",
    "    new_cfg = {\n",
    "        'lithology': cfg['lithology'],\n",
    "        'annotation': ann,\n",
    "        'dataset': ds_train,\n",
    "        'dataset_test': ds_test,\n",
    "        'labels_mapping': labels_mapping\n",
    "    }\n",
    "    \n",
    "    experiment[ppl_train].pipeline.set_config(new_cfg)\n",
    "    experiment[ppl_test].pipeline.set_config(new_cfg)\n",
    "    \n",
    "    import pickle\n",
    "    \n",
    "    with open(os.path.join(experiment[ppl_train].path, 'inputs.pkl'), 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'annotation': ann,\n",
    "            'dataset': ds_train,\n",
    "            'dataset_test': ds_test,\n",
    "            'labels_mapping': labels_mapping,\n",
    "            'reverse_mapping': reverse_mapping,\n",
    "            'counter': counter,\n",
    "            'weights': weights\n",
    "        }, f)\n",
    "\n",
    "\n",
    "def get_model(iteration, experiment, pipeline):\n",
    "    experiment[pipeline].pipeline.get_model_by_name('model').save(\n",
    "        os.path.join(experiment[pipeline].path, 'model.torch')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ppl = (Pipeline()\n",
    "    .add_namespace(utils)\n",
    "    .set_dataset(C('dataset_test'))\n",
    "    .load(uv=False, dst=['dl'])\n",
    "    .normalize(src='dl', dst='dl')\n",
    "    .add_namespace(np)\n",
    "    .create_labels(C('annotation').LITHOLOGY.loc)\n",
    "    .update(B('labels'), B('labels').tolist())\n",
    "    .to_array(src='dl', dst='images', dtype='float32')\n",
    "    .make_random_crops(src=['images'], dst=['images'], channels='last', shape=SHAPE[1:], n_crops=1)\n",
    "    .concatenate(B('images'), axis=0, save_to=B('images'))\n",
    "    .init_variable('metrics', default=None)\n",
    "    .transpose(B('images'), axes=(0, 3, 1, 2), save_to=B('images'))\n",
    "    .encode(B('labels'), C('labels_mapping'), save_to=B('labels'))\n",
    "    .import_model('model', C('import_from'))\n",
    "    .predict_model('model', B('images'), fetches='proba', save_to=B('proba'))\n",
    "    .gather_metrics('class', targets=B('labels'), predictions=B('proba'),\n",
    "                    fmt='proba', axis=-1, save_to=V('metrics', mode='u'))\n",
    "    .run_later(64, shuffle=False, n_epochs=1, bar=False, drop_last=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -r research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Research research is starting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distributor has 4 jobs\n"
     ]
    }
   ],
   "source": [
    "options = [\n",
    "    ['Песчаник', 'Аргиллит'],\n",
    "    ['Песчаник', 'Алевролит'],\n",
    "    ['Аргиллит', 'Алевролит'],\n",
    "    ['Аргиллит', 'Алевролит', 'Песчаник']\n",
    "]\n",
    "\n",
    "options = Option('lithology', [KV(item, '_'.join(item)) for item in options])\n",
    "\n",
    "research = (Research()\n",
    "            .add_grid(options)\n",
    "            .add_function(init_research, execute='#0', ppl_train='train', ppl_test='test',\n",
    "                          returns='mapping', name='init', logging=True)\n",
    "            .add_pipeline(train_ppl, variables='loss', name='train')\n",
    "            .add_pipeline(test_ppl, name='test', run=True, execute='last',\n",
    "                          import_from='train', variables='metrics', logging=True)\n",
    "            .get_metrics('test',\n",
    "                         metrics_var='metrics', \n",
    "                         metrics_name='f1_score',\n",
    "                         returns='f1_score',\n",
    "                         execute='last')\n",
    "            .add_function(get_model, execute='last', pipeline='train')\n",
    "           )\n",
    "\n",
    "research.run(1, None, workers=4, gpu=[0,1,2,3], bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "research.load_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'research/results/lithology_Аргиллит_Алевролит/0/'\n",
    "\n",
    "test_ppl = (Pipeline()\n",
    "    .add_namespace(utils)\n",
    "    .set_dataset(C('dataset_test'))\n",
    "    .load(uv=False, dst=['dl'])\n",
    "    .normalize(src='dl', dst='dl')\n",
    "    .add_namespace(np)\n",
    "    .create_labels(C('annotation').LITHOLOGY.loc)\n",
    "    .update(B('labels'), B('labels').tolist())\n",
    "    .to_array(src='dl', dst='images', dtype='float32')\n",
    "    .make_random_crops(src=['images'], dst=['images'], channels='last', shape=SHAPE[1:], n_crops=1)\n",
    "    .concatenate(B('images'), axis=0, save_to=B('images'))\n",
    "    .init_variable('metrics', default=None)\n",
    "    .transpose(B('images'), axes=(0, 3, 1, 2), save_to=B('images'))\n",
    "    .encode(B('labels'), C('labels_mapping'), save_to=B('labels'))\n",
    "    .init_model('dynamic', ResNet18, 'model', config={\n",
    "                'device': 'gpu:0', 'load/path': os.path.join(path, 'model.torch')\n",
    "    })\n",
    "    .predict_model('model', B('images'), fetches='proba', save_to=B('proba'))\n",
    "    .gather_metrics('class', targets=B('labels'), predictions=B('proba'),\n",
    "                    fmt='proba', axis=-1, save_to=V('metrics', mode='u'))\n",
    "    .run_later(64, shuffle=False, n_epochs=1, drop_last=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(path, 'inputs.pkl'), 'rb') as f:\n",
    "    inputs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation = inputs['annotation']\n",
    "ds = inputs['dataset_test']\n",
    "labels_mapping = inputs['labels_mapping']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ppl.set_config({\n",
    "    'annotation': annotation,\n",
    "    'dataset_test': ds,\n",
    "    'labels_mapping': labels_mapping\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ppl.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_metrics = test_ppl.get_variable('metrics')\n",
    "print(val_metrics._confusion_matrix)\n",
    "\n",
    "for m in ['specificity', 'sensitivity', 'accuracy', 'f1_score']:\n",
    "    print(m, ':', val_metrics.evaluate(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_mapping"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
