{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import time\n",
    "import collections\n",
    "from functools import reduce\n",
    "\n",
    "sys.path.insert(0, os.path.join(\"..\", \"..\", \"..\"))\n",
    "\n",
    "from petroflow import CoreBatch, CoreIndex\n",
    "from petroflow.batchflow.models.torch import ResNet18\n",
    "from petroflow.batchflow import Dataset, Pipeline, V, B, W, F, C, I, L\n",
    "from petroflow.batchflow.research import Research, Option, KV, Results\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '/notebooks/data/august_dataset/cropped_wells'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "N_EPOCH = 50\n",
    "SHAPE = (3, 500, 250)\n",
    "\n",
    "model_config = {'initial_block/inputs': 'images',\n",
    "                'inputs/images/shape': SHAPE,\n",
    "                'inputs/labels/classes': F(lambda x: len(x.pipeline.config['labels_mapping'])),\n",
    "                'initial_block/inputs': 'images',\n",
    "                'optimizer': 'Adam',\n",
    "                'output': ['proba', 'labels'],\n",
    "                'device': C('device'),\n",
    "                'loss': 'ce'#, dict(weight=V('weight')))}\n",
    "               }\n",
    "\n",
    "train_ppl = (Pipeline()\n",
    "    .add_namespace(utils)\n",
    "    .set_dataset(C('dataset'))\n",
    "    .load(uv=False, dst=['dl'])\n",
    "    .normalize(src='dl', dst='dl')\n",
    "    .add_namespace(np)\n",
    "    .create_labels(C('annotation').LITHOLOGY.loc)\n",
    "    .update(B('labels'), B('labels').tolist())\n",
    "    .to_array(src='dl', dst='images', dtype='float32')\n",
    "    .make_random_crops(src=['images'], dst=['images'], channels='last', shape=SHAPE[1:], n_crops=1)\n",
    "    .concatenate(B('images'), axis=0, save_to=B('images'))\n",
    "    .init_variable('loss', default=None)\n",
    "    .transpose(B('images'), axes=(0, 3, 1, 2), save_to=B('images'))\n",
    "    .encode(B('labels'), C('labels_mapping'), save_to=B('labels'))\n",
    "    .init_model('dynamic', ResNet18, 'model', model_config)\n",
    "    .train_model('model', B('images'), B('labels'), use_lock=True, fetches='loss',\n",
    "             save_to=V('loss', mode='w'))\n",
    "    .run_later(64, n_epochs=100, shuffle=42, drop_last=True)\n",
    ")\n",
    "\n",
    "def init_research(iteration, experiment, ppl_train, ppl_test):\n",
    "    ppl = experiment[ppl_train].pipeline\n",
    "    cfg = experiment[ppl_train].config.config()\n",
    "    ann, ds_train, ds_test, _ = utils.get_input_data(PATH, cfg['lithology'])\n",
    "    labels_mapping, reverse_mapping, counter, weights = utils.get_statistics(ds_train, ann)\n",
    "    #labels_mapping = {v: k for k,v in enumerate(cfg['lithology'])}\n",
    "\n",
    "    new_cfg = {\n",
    "        'lithology': cfg['lithology'],\n",
    "        'annotation': ann,\n",
    "        'dataset': ds_train,\n",
    "        'dataset_test': ds_test,\n",
    "        'labels_mapping': labels_mapping\n",
    "    }\n",
    "    \n",
    "    experiment[ppl_train].pipeline.set_config(new_cfg)\n",
    "    experiment[ppl_test].pipeline.set_config(new_cfg)\n",
    "    \n",
    "    import pickle\n",
    "    \n",
    "    with open(os.path.join(experiment[ppl_train].path, 'inputs.pkl'), 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'annotation': ann,\n",
    "            'dataset': ds_train,\n",
    "            'dataset_test': ds_test,\n",
    "            'labels_mapping': labels_mapping,\n",
    "            'reverse_mapping': reverse_mapping,\n",
    "            'counter': counter,\n",
    "            'weights': weights\n",
    "        }, f)\n",
    "\n",
    "\n",
    "def get_model(iteration, experiment, pipeline):\n",
    "    experiment[pipeline].pipeline.get_model_by_name('model').save(\n",
    "        os.path.join(experiment[pipeline].path, 'model.torch')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ppl = (Pipeline()\n",
    "    .add_namespace(utils)\n",
    "    .set_dataset(C('dataset_test'))\n",
    "    .load(uv=False, dst=['dl'])\n",
    "    .normalize(src='dl', dst='dl')\n",
    "    .add_namespace(np)\n",
    "    .create_labels(C('annotation').LITHOLOGY.loc)\n",
    "    .update(B('labels'), B('labels').tolist())\n",
    "    .to_array(src='dl', dst='images', dtype='float32')\n",
    "    .make_random_crops(src=['images'], dst=['images'], channels='last', shape=SHAPE[1:], n_crops=1)\n",
    "    .concatenate(B('images'), axis=0, save_to=B('images'))\n",
    "    .init_variable('metrics', default=None)\n",
    "    .transpose(B('images'), axes=(0, 3, 1, 2), save_to=B('images'))\n",
    "    .encode(B('labels'), C('labels_mapping'), save_to=B('labels'))\n",
    "    .import_model('model', C('import_from'))\n",
    "    .predict_model('model', B('images'), fetches='proba', save_to=B('proba'))\n",
    "    .gather_metrics('class', targets=B('labels'), predictions=B('proba'),\n",
    "                    fmt='proba', axis=-1, save_to=V('metrics', mode='u'))\n",
    "    .run_later(64, shuffle=False, n_epochs=1, bar=False, drop_last=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! rm -r research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = [\n",
    "    ['Песчаник', 'Аргиллит'],\n",
    "    ['Песчаник', 'Алевролит'],\n",
    "    ['Аргиллит', 'Алевролит'],\n",
    "#    ['Аргиллит', 'Алевролит', 'Песчаник']\n",
    "]\n",
    "\n",
    "options = Option('lithology', [KV(item, '_'.join(item)) for item in options])\n",
    "\n",
    "research = (Research()\n",
    "            .add_grid(options)\n",
    "            .add_function(init_research, execute='#0', ppl_train='train', ppl_test='test',\n",
    "                          returns='mapping', name='init', logging=True)\n",
    "            .add_pipeline(train_ppl, variables='loss', name='train')\n",
    "            .add_pipeline(test_ppl, name='test', run=True, execute='last',\n",
    "                          import_from='train', variables='metrics', logging=True)\n",
    "            .get_metrics('test',\n",
    "                         metrics_var='metrics', \n",
    "                         metrics_name='f1_score',\n",
    "                         returns='f1_score',\n",
    "                         execute='last')\n",
    "            .add_function(get_model, execute='last', pipeline='train')\n",
    "           )\n",
    "\n",
    "# research.run(1, None, name='research', workers=4, gpu=[0,1,2,3], bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6653386454183267"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Results('research').load(lithology='Аргиллит_Алевролит', names=['test']).metrics.iloc[0].evaluate('accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "lithology = 'Аргиллит_Алевролит'\n",
    "\n",
    "def get_lithology(lithology):\n",
    "    path = 'research/results/lithology_{}/0/'.format(lithology)\n",
    "\n",
    "    with open(os.path.join(path, 'inputs.pkl'), 'rb') as f:\n",
    "        inputs = pickle.load(f)\n",
    "\n",
    "    annotation = inputs['annotation']\n",
    "    ds = inputs['dataset_test']\n",
    "    labels_mapping = inputs['labels_mapping']\n",
    "\n",
    "    test_ppl = (Pipeline()\n",
    "        .add_namespace(utils)\n",
    "        .set_dataset(C('dataset_test'))\n",
    "        .load(uv=False, dst=['dl'])\n",
    "        .normalize(src='dl', dst='dl')\n",
    "        .add_namespace(np)\n",
    "        .create_labels(C('annotation').LITHOLOGY.loc)\n",
    "        .update(B('labels'), B('labels').tolist())\n",
    "        .to_array(src='dl', dst='images', dtype='float32')\n",
    "        .make_random_crops(src=['images'], dst=['images'], channels='last', shape=SHAPE[1:], n_crops=1)\n",
    "        .concatenate(B('images'), axis=0, save_to=B('images'))\n",
    "        .init_variable('metrics', default=None)\n",
    "        .init_variable('predictions', default=[])\n",
    "        .transpose(B('images'), axes=(0, 3, 1, 2), save_to=B('images'))\n",
    "        .encode(B('labels'), C('labels_mapping'), save_to=B('labels'))\n",
    "        .init_model('dynamic', ResNet18, 'model', config={\n",
    "                    'device': 'gpu:0', 'load/path': os.path.join(path, 'model.torch')\n",
    "        })\n",
    "        .predict_model('model', B('images'), fetches='proba', save_to=B('proba'))\n",
    "        .update(V('predictions', mode='a'), B('proba').argmax(axis=1))\n",
    "        .init_variable('ind', default=[])\n",
    "        .update(V('ind', mode='a'), B().indices)\n",
    "        .run_later(64, shuffle=False, n_epochs=1, drop_last=False)\n",
    "    )\n",
    "\n",
    "    test_ppl.set_config({\n",
    "        'annotation': annotation,\n",
    "        'dataset_test': ds,\n",
    "        'labels_mapping': labels_mapping\n",
    "    })\n",
    "\n",
    "    test_ppl.run()\n",
    "\n",
    "    df = annotation.loc[ds.indices]\n",
    "    df['pred'] = [inputs['reverse_mapping'][item] for item in np.concatenate(test_ppl.v('predictions'))]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = dict()\n",
    "for lithology in [\n",
    "    ['Песчаник', 'Аргиллит'],\n",
    "    ['Песчаник', 'Алевролит'],\n",
    "    ['Аргиллит', 'Алевролит'],\n",
    "]:\n",
    "    lithology = '_'.join(lithology)\n",
    "    res[lithology] = get_lithology(lithology)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = res['Аргиллит_Алевролит']\n",
    "df_true = df[df.LITHOLOGY == df.pred]\n",
    "arg1 = df_true[df_true.LITHOLOGY == 'Аргиллит'].index\n",
    "\n",
    "df = res['Песчаник_Аргиллит']\n",
    "df_true = df[df.LITHOLOGY == df.pred]\n",
    "arg2 = df_true[df_true.LITHOLOGY == 'Аргиллит'].index\n",
    "\n",
    "true_arg = np.intersect1d(arg1, arg2)\n",
    "\n",
    "df = res['Аргиллит_Алевролит']\n",
    "df_true = df[df.LITHOLOGY == df.pred]\n",
    "arg1 = df_true[df_true.LITHOLOGY == 'Алевролит'].index\n",
    "\n",
    "df = res['Песчаник_Алевролит']\n",
    "df_true = df[df.LITHOLOGY == df.pred]\n",
    "arg2 = df_true[df_true.LITHOLOGY == 'Алевролит'].index\n",
    "\n",
    "true_alev = np.intersect1d(arg1, arg2)\n",
    "\n",
    "df = res['Песчаник_Аргиллит']\n",
    "df_true = df[df.LITHOLOGY == df.pred]\n",
    "arg1 = df_true[df_true.LITHOLOGY == 'Песчаник'].index\n",
    "\n",
    "df = res['Песчаник_Алевролит']\n",
    "df_true = df[df.LITHOLOGY == df.pred]\n",
    "arg2 = df_true[df_true.LITHOLOGY == 'Песчаник'].index\n",
    "\n",
    "true_sand = np.intersect1d(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = res['Аргиллит_Алевролит']\n",
    "df_true = df[df.LITHOLOGY != df.pred]\n",
    "arg1 = df_true[df_true.LITHOLOGY == 'Аргиллит'].index\n",
    "\n",
    "df = res['Песчаник_Аргиллит']\n",
    "df_true = df[df.LITHOLOGY != df.pred]\n",
    "arg2 = df_true[df_true.LITHOLOGY == 'Аргиллит'].index\n",
    "\n",
    "wrong_arg = np.union1d(arg1, arg2)\n",
    "\n",
    "df = res['Аргиллит_Алевролит']\n",
    "df_true = df[df.LITHOLOGY != df.pred]\n",
    "arg1 = df_true[df_true.LITHOLOGY == 'Алевролит'].index\n",
    "\n",
    "df = res['Песчаник_Алевролит']\n",
    "df_true = df[df.LITHOLOGY != df.pred]\n",
    "arg2 = df_true[df_true.LITHOLOGY == 'Алевролит'].index\n",
    "\n",
    "wrong_alev = np.union1d(arg1, arg2)\n",
    "\n",
    "df = res['Песчаник_Аргиллит']\n",
    "df_true = df[df.LITHOLOGY != df.pred]\n",
    "arg1 = df_true[df_true.LITHOLOGY == 'Песчаник'].index\n",
    "\n",
    "df = res['Песчаник_Алевролит']\n",
    "df_true = df[df.LITHOLOGY != df.pred]\n",
    "arg2 = df_true[df_true.LITHOLOGY == 'Песчаник'].index\n",
    "\n",
    "wrong_sand = np.union1d(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [\n",
    "    pd.DataFrame({'SAMPLE': np.random.choice(true_arg, min(len(true_arg), 50), replace=False), 'LABEL': 'GOOD_ARG'}),\n",
    "    pd.DataFrame({'SAMPLE': np.random.choice(true_alev, min(len(true_alev), 50), replace=False), 'LABEL': 'GOOD_ALEV'}),\n",
    "    pd.DataFrame({'SAMPLE': np.random.choice(true_sand, min(len(true_sand), 50), replace=False), 'LABEL': 'GOOD_SAND'}),\n",
    "    pd.DataFrame({'SAMPLE': np.random.choice(wrong_arg, min(len(wrong_arg), 50), replace=False), 'LABEL': 'BAD_ARG'}),\n",
    "    pd.DataFrame({'SAMPLE': np.random.choice(wrong_alev, min(len(wrong_alev), 50), replace=False), 'LABEL': 'BAD_ALEV'}),\n",
    "    pd.DataFrame({'SAMPLE': np.random.choice(wrong_sand, min(len(wrong_sand), 50), replace=False), 'LABEL': 'BAD_SAND'}),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.concat(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('examples.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "300it [00:23, 12.73it/s]\n"
     ]
    }
   ],
   "source": [
    "import PIL, tqdm\n",
    "\n",
    "for i, item in tqdm.tqdm(enumerate(results.SAMPLE)):\n",
    "    src = os.path.join(PATH, '_'.join(item.split('_')[:2]), 'samples_dl','_'.join(item.split('_')[2:]))\n",
    "    dst = os.path.join('/notebooks/data/august_dataset/examples', '_'.join(item.split('_')[2:]))\n",
    "    img = PIL.Image.open(src)\n",
    "    pos = np.random.choice(img.size[1] - 500)\n",
    "    img = img.crop(box=((0, pos, 250, pos+500)))\n",
    "    img.save(dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2925"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! mkdir '/notebooks/data/august_dataset/examples'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(os.listdir('/notebooks/data/august_dataset/examples'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_metrics = test_ppl.get_variable('metrics')\n",
    "# print(val_metrics._confusion_matrix)\n",
    "\n",
    "# for m in ['specificity', 'sensitivity', 'accuracy', 'f1_score']:\n",
    "#     print(m, ':', val_metrics.evaluate(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import PIL\n",
    "# import cv2\n",
    "\n",
    "# image = PIL.Image.open(os.path.join(PATH, '9281_новопортовское', 'samples_dl', '2048.69_2049.19.png',))\n",
    "# image = np.array(image).astype('uint8')\n",
    "# print(image.ndim)\n",
    "# # cv2.equalizeHist(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bad = ['5144_новопортовское_2287.13_2288.04.png',\n",
    "#        '9281_новопортовское_2048.69_2049.19.png',\n",
    "#        '115_восточно-мессояхское_2454.25_2457.2.png',\n",
    "#        '115_восточно-мессояхское_2454.25_2457.2.png']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<petroflow.batchflow.batchflow.pipeline.Pipeline at 0x7fa675034ac8>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# index = inputs['dataset'].index.create_subset(np.array(bad))\n",
    "# ds = Dataset(index, batch_class=CoreBatch)\n",
    "\n",
    "def my_print(x):\n",
    "    if x != 64:\n",
    "        print(x)\n",
    "\n",
    "train_ppl = (Pipeline()\n",
    "    .init_variable('ind', None)\n",
    "    .my_print(L(len)(L(np.unique)(B().indices)))\n",
    "    .update(V('ind'), B().indices)\n",
    "    .add_namespace(utils)\n",
    "    .set_dataset(C('dataset'))\n",
    "#     .load(uv=False, dst=['dl'])\n",
    "#     .normalize(src='dl', dst='dl')\n",
    "#     .add_namespace(np)\n",
    "#     .create_labels(C('annotation').LITHOLOGY.loc)\n",
    "#     .update(B('labels'), B('labels').tolist())\n",
    "#     .to_array(src='dl', dst='images', dtype='float32')\n",
    "#     .make_random_crops(src=['images'], dst=['images'], channels='last', shape=SHAPE[1:], n_crops=1)\n",
    "#     .concatenate(B('images'), axis=0, save_to=B('images'))\n",
    "#     .init_variable('loss', default=None)\n",
    "#     .transpose(B('images'), axes=(0, 3, 1, 2), save_to=B('images'))\n",
    "#     .encode(B('labels'), C('labels_mapping'), save_to=B('labels'))\n",
    "#     .init_model('dynamic', ResNet18, 'model', model_config)\n",
    "#     .train_model('model', B('images'), B('labels'), use_lock=True, fetches='loss',\n",
    "#              save_to=V('loss', mode='w'))\n",
    "    .run_later(64, n_epochs=100, bar=False, shuffle=42, drop_last=True)\n",
    ")\n",
    "\n",
    "train_ppl.set_config({\n",
    "    'annotation': inputs['annotation'],\n",
    "    'dataset': inputs['dataset'],\n",
    "    'labels_mapping': inputs['labels_mapping'],\n",
    "    'device': 'gpu:0'\n",
    "})\n",
    "\n",
    "train_ppl.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1250, 250, 3), (2275, 250, 3)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [np.array(img).shape for img in b.dl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 2}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "set(list(Counter(train_ppl.v('ind')).values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
